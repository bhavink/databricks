{"cells":[{"cell_type":"markdown","source":["### Accessing ADLS\n\n- #### [Access directly using storage account key](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key)\n- #### Access using service principal\n    - [Directly](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#create-and-grant-permissions-to-service-principal) or\n    - [Mount](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--mount-an-azure-data-lake-storage-gen2-account-using-a-service-principal-and-oauth-20) points\n      - mount points available to all the users\n    - #### Best suited for:    \n     - Ideal for running jobs, automated workloads\n     - Best suited for auto-pilot data-eng workloads\n- #### [AAD credential pass thru](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#---access-automatically-with-your-azure-active-directory-credentials)\n  - Ideal for interactive analytical workloads\n  - Supports mount points\n  - Available for ADLS Gen2, PowerBI, Synapse DW"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"897b364d-3428-4f94-aead-a2534be3e2f6"}}},{"cell_type":"markdown","source":["#### Download these notebooks from my [github](https://github.com/bhavink/databricks/tree/master/adb4u) repo\n`https://github.com/bhavink/databricks/tree/master/adb4u`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082cf616-2a7a-4223-bccf-f4618826e2a5"}}},{"cell_type":"code","source":["%run ./secrets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bbe1ee5-e50a-40fe-b3dd-475f3e675a1d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Access directly using storage account key\n\n- Open to whosoever has the storage access key\n- No data access controls / restrictions\n- Quick and Easy\n- Not recommended\n\n#### You'll need\n- Data Lake Storage account and container name\n- ADLS Gen2 account access key"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b61fb22-a510-4fa3-8c1a-5e174474981c"}}},{"cell_type":"code","source":["%scala\nspark.conf.set(\n  \"fs.azure.account.key.labsdatalake.dfs.core.windows.net\",labsdatalake_key)\n\nspark.sparkContext.hadoopConfiguration.set(\n  \"fs.azure.account.key.labsdatalake.dfs.core.windows.net\",labsdatalake_key)\n\ndisplay(dbutils.fs.ls(\"abfss://container@labsdatalake.dfs.core.windows.net/listonly/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Access directly using storage account key","showTitle":true,"inputWidgets":{},"nuid":"8e90951c-d1ba-4971-bbc3-cb9c4b3d4a31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv","chick_menu.csv",873]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv</td><td>chick_menu.csv</td><td>873</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">labsdatalake_key: String = &lt;lazy&gt;\nlabs_sp_client_id: String = &lt;lazy&gt;\nlabs_sp_client_key: String = &lt;lazy&gt;\ntenant_id: String = &lt;lazy&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">labsdatalake_key: String = &lt;lazy&gt;\nlabs_sp_client_id: String = &lt;lazy&gt;\nlabs_sp_client_key: String = &lt;lazy&gt;\ntenant_id: String = &lt;lazy&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### You'll need\n- Data Lake Storage account and container name\n- Azure Tenant Id\n- Service Principal Client Id\n- Service Principal Client Secret\n- Add Service Principal or the group that this principal belongs to, to the data lake storage account with appropriate role\n  - Stroage Blob Data Reader\n  - Storage Blob Data Owner\n  - Storage Blob Data Contributor"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Access ADLS Gen2 directly using service principal keys","showTitle":true,"inputWidgets":{},"nuid":"0a460816-f173-4bfd-b76d-8f64da458592"}}},{"cell_type":"code","source":["%scala\nspark.conf.set(\"fs.azure.account.auth.type.labsdatalake.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.labsdatalake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.labsdatalake.dfs.core.windows.net\", labs_sp_client_id)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.labsdatalake.dfs.core.windows.net\",labs_sp_client_key)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.labsdatalake.dfs.core.windows.net\", s\"https://login.microsoftonline.com/$tenant_id/oauth2/token\")\n\n\nspark.sparkContext.hadoopConfiguration.set(\"fs.azure.account.auth.type.labsdatalake.dfs.core.windows.net\", \"OAuth\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.azure.account.oauth.provider.type.labsdatalake.dfs.core.windows.net\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.sparkContext.hadoopConfiguration.set(\"fs.azure.account.oauth2.client.id.labsdatalake.dfs.core.windows.net\", labs_sp_client_id)\nspark.sparkContext.hadoopConfiguration.set(\"fs.azure.account.oauth2.client.secret.labsdatalake.dfs.core.windows.net\", labs_sp_client_key)\nspark.sparkContext.hadoopConfiguration.set(\"fs.azure.account.oauth2.client.endpoint.labsdatalake.dfs.core.windows.net\", s\"https://login.microsoftonline.com/$tenant_id/oauth2/token\")\n\n\ndisplay(dbutils.fs.ls(\"abfss://container@labsdatalake.dfs.core.windows.net/listonly/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Access ADLS Gen2 directly using service principal keys","showTitle":true,"inputWidgets":{},"nuid":"1d44d015-ba65-44df-8ab1-a764cf670400"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv","chick_menu.csv",873]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv</td><td>chick_menu.csv</td><td>873</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%python\ndf = spark.sql(\"select * from csv.`abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv`\")\ndisplay(df)\n\ndisplay(dbutils.fs.ls(\"abfss://container@labsdatalake.dfs.core.windows.net/listonly/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cfe30ad-c4a4-420e-8713-8a1428003892"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["SKU","NAME"],["90002","Grilled Chicken Cool Wrap"],["90003","Grilled Nuggets"],["90010","Egg White Grill"],["90011","Chicken Noodle Soup"],["90018","Chicken, Egg and Cheese Bagel"],["90019","Chicken Nuggets"],["90026","Hash Browns"],["90027","Greek Yogurt Parfait"],["90000","Sausage/Bacon, Egg and Cheese Biscuit"],["90001","Sausage/Bacon, Egg and Cheese Muffin"],["90008","Chicken Salad Sandwich"],["90009","Chick-n-Strips"],["90016","Deluxe Chicken Sandwich"],["90017","Chicken Biscuit"],["90024","Milkshake"],["90025","Soft Drink"],["90004","Grilled Chicken Sandwich"],["90005","Grilled Market Salad"],["90006","Cobb Salad"],["90007","Spicy Southwest Salad"],["90012","Chicken Minis"],["90013","Breakfast Burrito (with Chicken)"],["90014","Chicken Sandwich"],["90015","Grilled Chicken Club Sandwich"],["90020","Spicy Chicken Sandwich"],["90021","Spicy Deluxe Chicken Sandwich"],["90022","Frosted Lemonade"],["90023","Frosted Coffee"],["90028","Superfood"],["90029","Chocolate Chunk Cookie"],["90030","Waffle Potato Fries"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th></tr></thead><tbody><tr><td>SKU</td><td>NAME</td></tr><tr><td>90002</td><td>Grilled Chicken Cool Wrap</td></tr><tr><td>90003</td><td>Grilled Nuggets</td></tr><tr><td>90010</td><td>Egg White Grill</td></tr><tr><td>90011</td><td>Chicken Noodle Soup</td></tr><tr><td>90018</td><td>Chicken, Egg and Cheese Bagel</td></tr><tr><td>90019</td><td>Chicken Nuggets</td></tr><tr><td>90026</td><td>Hash Browns</td></tr><tr><td>90027</td><td>Greek Yogurt Parfait</td></tr><tr><td>90000</td><td>Sausage/Bacon, Egg and Cheese Biscuit</td></tr><tr><td>90001</td><td>Sausage/Bacon, Egg and Cheese Muffin</td></tr><tr><td>90008</td><td>Chicken Salad Sandwich</td></tr><tr><td>90009</td><td>Chick-n-Strips</td></tr><tr><td>90016</td><td>Deluxe Chicken Sandwich</td></tr><tr><td>90017</td><td>Chicken Biscuit</td></tr><tr><td>90024</td><td>Milkshake</td></tr><tr><td>90025</td><td>Soft Drink</td></tr><tr><td>90004</td><td>Grilled Chicken Sandwich</td></tr><tr><td>90005</td><td>Grilled Market Salad</td></tr><tr><td>90006</td><td>Cobb Salad</td></tr><tr><td>90007</td><td>Spicy Southwest Salad</td></tr><tr><td>90012</td><td>Chicken Minis</td></tr><tr><td>90013</td><td>Breakfast Burrito (with Chicken)</td></tr><tr><td>90014</td><td>Chicken Sandwich</td></tr><tr><td>90015</td><td>Grilled Chicken Club Sandwich</td></tr><tr><td>90020</td><td>Spicy Chicken Sandwich</td></tr><tr><td>90021</td><td>Spicy Deluxe Chicken Sandwich</td></tr><tr><td>90022</td><td>Frosted Lemonade</td></tr><tr><td>90023</td><td>Frosted Coffee</td></tr><tr><td>90028</td><td>Superfood</td></tr><tr><td>90029</td><td>Chocolate Chunk Cookie</td></tr><tr><td>90030</td><td>Waffle Potato Fries</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv","chick_menu.csv",873]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv</td><td>chick_menu.csv</td><td>873</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\nselect * from csv.`abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7acdf9af-7968-4137-85ab-3a7982142ff3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["SKU","NAME"],["90002","Grilled Chicken Cool Wrap"],["90003","Grilled Nuggets"],["90010","Egg White Grill"],["90011","Chicken Noodle Soup"],["90018","Chicken, Egg and Cheese Bagel"],["90019","Chicken Nuggets"],["90026","Hash Browns"],["90027","Greek Yogurt Parfait"],["90000","Sausage/Bacon, Egg and Cheese Biscuit"],["90001","Sausage/Bacon, Egg and Cheese Muffin"],["90008","Chicken Salad Sandwich"],["90009","Chick-n-Strips"],["90016","Deluxe Chicken Sandwich"],["90017","Chicken Biscuit"],["90024","Milkshake"],["90025","Soft Drink"],["90004","Grilled Chicken Sandwich"],["90005","Grilled Market Salad"],["90006","Cobb Salad"],["90007","Spicy Southwest Salad"],["90012","Chicken Minis"],["90013","Breakfast Burrito (with Chicken)"],["90014","Chicken Sandwich"],["90015","Grilled Chicken Club Sandwich"],["90020","Spicy Chicken Sandwich"],["90021","Spicy Deluxe Chicken Sandwich"],["90022","Frosted Lemonade"],["90023","Frosted Coffee"],["90028","Superfood"],["90029","Chocolate Chunk Cookie"],["90030","Waffle Potato Fries"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th></tr></thead><tbody><tr><td>SKU</td><td>NAME</td></tr><tr><td>90002</td><td>Grilled Chicken Cool Wrap</td></tr><tr><td>90003</td><td>Grilled Nuggets</td></tr><tr><td>90010</td><td>Egg White Grill</td></tr><tr><td>90011</td><td>Chicken Noodle Soup</td></tr><tr><td>90018</td><td>Chicken, Egg and Cheese Bagel</td></tr><tr><td>90019</td><td>Chicken Nuggets</td></tr><tr><td>90026</td><td>Hash Browns</td></tr><tr><td>90027</td><td>Greek Yogurt Parfait</td></tr><tr><td>90000</td><td>Sausage/Bacon, Egg and Cheese Biscuit</td></tr><tr><td>90001</td><td>Sausage/Bacon, Egg and Cheese Muffin</td></tr><tr><td>90008</td><td>Chicken Salad Sandwich</td></tr><tr><td>90009</td><td>Chick-n-Strips</td></tr><tr><td>90016</td><td>Deluxe Chicken Sandwich</td></tr><tr><td>90017</td><td>Chicken Biscuit</td></tr><tr><td>90024</td><td>Milkshake</td></tr><tr><td>90025</td><td>Soft Drink</td></tr><tr><td>90004</td><td>Grilled Chicken Sandwich</td></tr><tr><td>90005</td><td>Grilled Market Salad</td></tr><tr><td>90006</td><td>Cobb Salad</td></tr><tr><td>90007</td><td>Spicy Southwest Salad</td></tr><tr><td>90012</td><td>Chicken Minis</td></tr><tr><td>90013</td><td>Breakfast Burrito (with Chicken)</td></tr><tr><td>90014</td><td>Chicken Sandwich</td></tr><tr><td>90015</td><td>Grilled Chicken Club Sandwich</td></tr><tr><td>90020</td><td>Spicy Chicken Sandwich</td></tr><tr><td>90021</td><td>Spicy Deluxe Chicken Sandwich</td></tr><tr><td>90022</td><td>Frosted Lemonade</td></tr><tr><td>90023</td><td>Frosted Coffee</td></tr><tr><td>90028</td><td>Superfood</td></tr><tr><td>90029</td><td>Chocolate Chunk Cookie</td></tr><tr><td>90030</td><td>Waffle Potato Fries</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nval df = spark.read.csv(\"abfss://container@labsdatalake.dfs.core.windows.net/listonly/chick_menu.csv\")\ndf.write.format(\"parquet\").save(\"abfss://container@labsdatalake.dfs.core.windows.net/listonly/data.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"This will fail as I try to write to a dir using \"reader\" role","showTitle":true,"inputWidgets":{},"nuid":"82f93dfb-4402-4f72-b8a2-fd63b453d112"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:230)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:116)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:423)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:296)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:2)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:47)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:49)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:51)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw.&lt;init&gt;(command-3842029948742175:53)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw.&lt;init&gt;(command-3842029948742175:55)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read.&lt;init&gt;(command-3842029948742175:57)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;init&gt;(command-3842029948742175:61)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;clinit&gt;(command-3842029948742175)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print(&lt;notebook&gt;:6)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 4 times, most recent failure: Lost task 0.3 in stage 20.0 (TID 35, 172.1.0.72, executor 0): PUT https://labsdatalake.dfs.core.windows.net/container/listonly/data.csv/_started_988672583398226360?resource=file&amp;timeout=90\nStatusCode=403\nStatusDescription=This request is not authorized to perform this operation using this permission.\nErrorCode=AuthorizationPermissionMismatch\nErrorMessage=This request is not authorized to perform this operation using this permission.\nRequestId:404d0a68-e01f-0004-7b9a-ee5723000000\nTime:2021-01-19T19:37:21.1676508Z\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:244)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:322)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:220)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:268)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:209)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:642)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:645)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:199)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:116)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:423)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:296)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:2)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:47)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:49)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:51)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw.&lt;init&gt;(command-3842029948742175:53)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw.&lt;init&gt;(command-3842029948742175:55)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read.&lt;init&gt;(command-3842029948742175:57)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;init&gt;(command-3842029948742175:61)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;clinit&gt;(command-3842029948742175)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print(&lt;notebook&gt;:6)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: PUT https://labsdatalake.dfs.core.windows.net/container/listonly/data.csv/_started_988672583398226360?resource=file&amp;timeout=90\nStatusCode=403\nStatusDescription=This request is not authorized to perform this operation using this permission.\nErrorCode=AuthorizationPermissionMismatch\nErrorMessage=This request is not authorized to perform this operation using this permission.\nRequestId:404d0a68-e01f-0004-7b9a-ee5723000000\nTime:2021-01-19T19:37:21.1676508Z\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:244)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:322)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:220)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:268)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:209)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:642)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:645)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)</div>","errorSummary":"Job aborted.\nCaused by: Job aborted due to stage failure.\nCaused by: AbfsRestOperationException: PUT https://labsdatalake.dfs.core.windows.net/container/listonly/data.csv/_started_988672583398226360?resource=file&timeout=90\nStatusCode=403\nStatusDescription=This request is not authorized to perform this operation using this permission.\nErrorCode=AuthorizationPermissionMismatch\nErrorMessage=This request is not authorized to perform this operation using this permission.\nRequestId:404d0a68-e01f-0004-7b9a-ee5723000000\nTime:2021-01-19T19:37:21.1676508Z","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:230)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:116)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:423)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:296)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:2)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:47)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:49)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:51)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw.&lt;init&gt;(command-3842029948742175:53)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw.&lt;init&gt;(command-3842029948742175:55)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read.&lt;init&gt;(command-3842029948742175:57)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;init&gt;(command-3842029948742175:61)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;clinit&gt;(command-3842029948742175)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print(&lt;notebook&gt;:6)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 4 times, most recent failure: Lost task 0.3 in stage 20.0 (TID 35, 172.1.0.72, executor 0): PUT https://labsdatalake.dfs.core.windows.net/container/listonly/data.csv/_started_988672583398226360?resource=file&amp;timeout=90\nStatusCode=403\nStatusDescription=This request is not authorized to perform this operation using this permission.\nErrorCode=AuthorizationPermissionMismatch\nErrorMessage=This request is not authorized to perform this operation using this permission.\nRequestId:404d0a68-e01f-0004-7b9a-ee5723000000\nTime:2021-01-19T19:37:21.1676508Z\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:244)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:322)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:220)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:268)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:209)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:642)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:645)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:199)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:116)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:841)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1018)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:423)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:296)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:2)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:47)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:49)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw$$iw.&lt;init&gt;(command-3842029948742175:51)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw$$iw.&lt;init&gt;(command-3842029948742175:53)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$$iw.&lt;init&gt;(command-3842029948742175:55)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read.&lt;init&gt;(command-3842029948742175:57)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;init&gt;(command-3842029948742175:61)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$read$.&lt;clinit&gt;(command-3842029948742175)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval$.$print(&lt;notebook&gt;:6)\n\tat line47a68c3f56e74ebb8dbc291f95c2f1a743.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:233)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:773)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:726)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: PUT https://labsdatalake.dfs.core.windows.net/container/listonly/data.csv/_started_988672583398226360?resource=file&amp;timeout=90\nStatusCode=403\nStatusDescription=This request is not authorized to perform this operation using this permission.\nErrorCode=AuthorizationPermissionMismatch\nErrorMessage=This request is not authorized to perform this operation using this permission.\nRequestId:404d0a68-e01f-0004-7b9a-ee5723000000\nTime:2021-01-19T19:37:21.1676508Z\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:244)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:322)\n\tat shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:220)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:268)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:209)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:642)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:645)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n\nval configs = Map(\n  \"fs.azure.account.auth.type\" -> \"OAuth\",\n  \"fs.azure.account.oauth.provider.type\" -> \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n  \"fs.azure.account.oauth2.client.id\" -> labs_sp_client_id,\n  \"fs.azure.account.oauth2.client.secret\" -> labs_sp_client_key,\n  \"fs.azure.account.oauth2.client.endpoint\" -> s\"https://login.microsoftonline.com/$tenant_id/oauth2/token\")\n\n\n// Optionally, you can add <directory-name> to the source URI of your mount point.\ndbutils.fs.mount(\n  source = \"abfss://container@labsdatalake.dfs.core.windows.net/\",\n  mountPoint = \"/mnt/labsdatalake/container\",\n  extraConfigs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Access ADLS Gen2 as a mount point on dbfs using service principal","showTitle":true,"inputWidgets":{},"nuid":"8204d36d-64ce-4fa3-9fb5-9c2ed475afd5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">configs: scala.collection.immutable.Map[String,String] = Map(fs.azure.account.oauth2.client.secret -&gt; w1zL070~nOXRPOFCwVz-0FJOKX44-v-SRc, fs.azure.account.auth.type -&gt; OAuth, fs.azure.account.oauth2.client.endpoint -&gt; https://login.microsoftonline.com/9f37a392-f0ae-4280-9796-f1864a10effc/oauth2/token, fs.azure.account.oauth.provider.type -&gt; org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider, fs.azure.account.oauth2.client.id -&gt; b593496c-7491-475f-b4aa-c639053115eb)\nres8: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">configs: scala.collection.immutable.Map[String,String] = Map(fs.azure.account.oauth2.client.secret -&gt; w1zL070~nOXRPOFCwVz-0FJOKX44-v-SRc, fs.azure.account.auth.type -&gt; OAuth, fs.azure.account.oauth2.client.endpoint -&gt; https://login.microsoftonline.com/9f37a392-f0ae-4280-9796-f1864a10effc/oauth2/token, fs.azure.account.oauth.provider.type -&gt; org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider, fs.azure.account.oauth2.client.id -&gt; b593496c-7491-475f-b4aa-c639053115eb)\nres8: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /mnt/labsdatalake/container/listonly/chick_menu.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f59f4d28-45bd-4ac3-8eea-e8bf8d332015"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/labsdatalake/container/listonly/chick_menu.csv","chick_menu.csv",873]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/labsdatalake/container/listonly/chick_menu.csv</td><td>chick_menu.csv</td><td>873</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### ADB with an [egress appliance](https://databricks.com/blog/2020/03/27/data-exfiltration-protection-with-azure-databricks.html) and ADLS access\n\n#### If you have a firewall in front of ADB then the access pattern would like this:\n\n  -  AAD Servcie endpoint enabled on ADB subnets\n  -  Storage service endpoint disabled on ADB subnets (yes you have to disable service endpoint)\n  -  Storage service endpoint enabled on subnet hosting firewall\n  -  Firewall subnet whitelisted on ADLS Gen2 --> Netowrking --> Firewall and Virtual Networks --> Selected Networks\n  -  As the firewall, ADB and ADLS are on Azure, traffic will stay on Azure backbone."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c338681-a3ec-4628-b47e-9e06e3b04d22"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TestADLS","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3842029948742159}},"nbformat":4,"nbformat_minor":0}
