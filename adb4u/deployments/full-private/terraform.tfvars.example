# ==============================================
# Full Private Deployment Configuration Example
# ==============================================
# This file provides example values for deploying an Azure Databricks workspace
# using the Full Private pattern (Private Link Control Plane + NPIP Data Plane).
#
# This is an AIR-GAPPED deployment with NO internet egress.
# Customer must provide their own package repositories (PyPI, Maven, etc.).
#
# Copy this file to terraform.tfvars and customize for your environment.
# ==============================================

# ==============================================
# Authentication Configuration
# ==============================================
# Terraform providers support multiple authentication methods.
# Choose the method that best fits your environment.

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ OPTION 1: Environment Variables (RECOMMENDED)                            â”‚
# â”‚ Best for: Development, CI/CD pipelines, credential security              â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Add to ~/.zshrc or ~/.bashrc:
#
# # Azure Authentication
# export ARM_CLIENT_ID="<service-principal-app-id>"
# export ARM_CLIENT_SECRET="<service-principal-secret>"
# export ARM_TENANT_ID="<azure-ad-tenant-id>"
# export ARM_SUBSCRIPTION_ID="<azure-subscription-id>"
#
# # Databricks Account Authentication
# export DATABRICKS_AZURE_TENANT_ID="$ARM_TENANT_ID"
# export TF_VAR_databricks_account_id="<databricks-account-id>"
#
# Then run: source ~/.zshrc

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ OPTION 2: Azure CLI (DEVELOPMENT ONLY)                                   â”‚
# â”‚ Best for: Local development, testing                                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Prerequisites:
# 1. Install Azure CLI: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli
# 2. Login: az login
# 3. Set subscription: az account set --subscription <subscription-id>
# 4. Verify: az account show
#
# âš ï¸ IMPORTANT for Full Private: You MUST have network connectivity to the VNet
#    (via VPN, ExpressRoute, or jump box) to access the workspace after deployment.

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ OPTION 3: Service Principal (PRODUCTION)                                 â”‚
# â”‚ Best for: Production, CI/CD, automated deployments                       â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Create Service Principal:
# az ad sp create-for-rbac \
#   --name "terraform-databricks-sp" \
#   --role "Contributor" \
#   --scopes /subscriptions/<subscription-id>
#
# Add User Access Administrator role (needed for RBAC):
# az role assignment create \
#   --assignee "<app-id>" \
#   --role "User Access Administrator" \
#   --scope /subscriptions/<subscription-id>
#
# Then set environment variables (see OPTION 1)

# ==============================================
# Core Configuration (REQUIRED)
# ==============================================

workspace_prefix = "prodpl"  # Lowercase alphanumeric, max 12 chars
location         = "eastus2"
resource_group_name = "rg-databricks-prod-pl-eastus2"

# Databricks Account ID (REQUIRED for Unity Catalog)
# Get from: https://accounts.azuredatabricks.net (top-right corner)
# Format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
databricks_account_id = "12345678-1234-1234-1234-123456789012"

# ==============================================
# Network Configuration
# ==============================================

# Option 1: Create New Network (Default)
use_existing_network          = false
vnet_address_space            = ["10.178.0.0/20"]
public_subnet_address_prefix  = ["10.178.0.0/26"]
private_subnet_address_prefix = ["10.178.1.0/26"]
privatelink_subnet_address_prefix = ["10.178.2.0/27"]  # For Private Endpoints

# Option 2: Bring Your Own VNet (BYOV)
# Uncomment and configure if using existing network:
# use_existing_network             = true
# existing_vnet_name               = "my-existing-vnet"
# existing_resource_group_name     = "my-existing-rg"
# existing_public_subnet_name      = "databricks-public"
# existing_private_subnet_name     = "databricks-private"
# existing_privatelink_subnet_name = "databricks-privatelink"
# existing_nsg_name                = "databricks-nsg"
#
# âš ï¸ IMPORTANT: Existing subnets MUST have:
#    - Public & Private: Databricks delegation (Microsoft.Databricks/workspaces)
#    - Private Link subnet: NO delegation, NO NAT Gateway
#    - NSG attached (for Private Link deployments, custom rules are required)

# ==============================================
# Customer-Managed Keys (RECOMMENDED for Full Private)
# ==============================================

# CMK is enabled by default for Full Private pattern
enable_cmk_managed_services = true
enable_cmk_managed_disks    = true
enable_cmk_dbfs_root        = true

# Provide your Key Vault and Key IDs
cmk_key_vault_key_id = "https://my-keyvault.vault.azure.net/keys/databricks-cmk/abc123"
cmk_key_vault_id     = "/subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.KeyVault/vaults/my-keyvault"

# âš ï¸ IMPORTANT: Your Key Vault MUST be accessible from your deployment machine
#    and have appropriate access policies for Databricks service principal.

# ==============================================
# Network Access Control
# ==============================================

# Public Network Access (CRITICAL FOR DEPLOYMENT)
# - true:  Allows public internet access (DEFAULT - needed for initial deployment from outside VNet)
# - false: Forces Private Link only (air-gapped - requires VPN/Bastion/Jump Box access)
#
# âš ï¸ RECOMMENDED WORKFLOW:
#   1. Initial deployment: enable_public_network_access = true (deploy from your laptop)
#   2. After deployment completes: Set to false and re-apply to lock down workspace
#   3. All future operations must be done from inside VNet (via VPN/Bastion/Jump Box)
enable_public_network_access = true

# ==============================================
# IP Access Lists (Optional - RECOMMENDED for Security)
# ==============================================

# RECOMMENDED: Use IP Access Lists with enable_public_network_access = true
# This provides security while maintaining deployment convenience.
#
# Add your network's public IP ranges:
enable_ip_access_lists = true
allowed_ip_ranges = [
  "203.0.113.0/24",        # Example: Corporate office network
  "198.51.100.45/32",      # Example: VPN endpoint
  # "YOUR_PUBLIC_IP/32",   # âš ï¸ REPLACE: Add your IP address
  # "YOUR_CORP_NETWORK/24" # âš ï¸ REPLACE: Add your corporate network CIDR
]

# To find your public IP: curl ifconfig.me
# Azure Portal users: Add Azure Portal IP ranges for your region
# See: https://www.microsoft.com/en-us/download/details.aspx?id=56519

# ==============================================
# Service Endpoint Policy (SEP) - Optional
# ==============================================

enable_service_endpoint_policy = true  # Set to true to enforce storage egress control for classic compute

# Important Note on Databricks System Storage Alias:
# For workspaces created ON or AFTER July 14, 2025, the '/services/Azure/Databricks' alias
# for Databricks-managed system storage (artifacts, logs, system tables) is supported by default.
# If your workspace was created BEFORE this date, you must contact your Databricks account team
# to enable this feature for your workspace. Otherwise, the deployment will fail.

# Optional: Additional customer storage accounts to allow
# additional_allowed_storage_ids = [
#   "/subscriptions/.../resourceGroups/.../providers/Microsoft.Storage/storageAccounts/mydatalake"
# ]

# ==============================================
# Network Connectivity Configuration (NCC)
# ==============================================

# NCC enables Databricks Serverless compute (SQL Warehouses, Serverless Notebooks)
# to access storage via Private Link from Control Plane.
#
# âš ï¸  IMPORTANT: Requires manual approval in Azure Portal
# 
# Workflow:
# 1. Initial deployment: enable_ncc = false (deploy workspace + classic compute)
# 2. Test with classic clusters (clusters run in your VNet, use your Private Endpoints)
# 3. When ready for serverless: Set enable_ncc = true
# 4. Run terraform apply (will timeout - EXPECTED)
# 5. Azure Portal > each Storage Account > Networking > Private Endpoint Connections
# 6. Approve all pending connections from Databricks
# 7. Run terraform apply again - completes successfully
#
# NOTE: Classic clusters work without NCC. Only needed for serverless compute.
enable_ncc = false

# ==============================================
# Unity Catalog Configuration
# ==============================================

# First-time deployment: Create new metastore
create_metastore = true
metastore_name   = "prod-pl-eastus2-metastore"

# Optional: Custom storage account name prefixes (lowercase, alphanumeric, max 18 chars)
# If not specified, defaults to: {workspace_prefix}metastore and {workspace_prefix}external
# Random suffix (6 chars) will be added automatically
metastore_storage_name_prefix = "ucrootstorage"    # Results in: ucrootstorage{random}
external_storage_name_prefix  = "ucextstorage"     # Results in: ucextstorage{random}

# Subsequent workspaces in same region: Reuse existing metastore
# create_metastore      = false
# existing_metastore_id = "abc123-def456-ghi789"

# Access Connector (Managed Identity)
# Option 1: Create per-workspace (Default - Recommended)
create_access_connector = true

# Option 2: Share existing Access Connector across workspaces
# create_access_connector               = false
# existing_access_connector_id          = "/subscriptions/.../resourceGroups/.../providers/Microsoft.Databricks/accessConnectors/shared-connector"
# existing_access_connector_principal_id = "abcd1234-ef56-7890-ghij-klmnopqrstuv"

# ==============================================
# Tags
# ==============================================

tag_owner     = "your.email@company.com"
tag_keepuntil = "12/31/2026"

tags = {
  Environment = "Production"
  ManagedBy   = "Terraform"
  Pattern     = "Full-Private"
  CostCenter  = "Engineering"
  Owner       = "data-platform-team@company.com"
  Security    = "Air-Gapped"
}

# ==============================================
# IMPORTANT NOTES FOR AIR-GAPPED DEPLOYMENT
# ==============================================
#
# âœ… What Works:
# - Workspace UI/API access (via Private Link from within VNet)
# - Cluster creation and execution
# - Unity Catalog and data access
# - All storage access via Private Link
#
# âŒ What Doesn't Work (No Internet Egress):
# - Downloading packages from PyPI (pip install)
# - Downloading libraries from Maven Central
# - Pulling Docker images from Docker Hub
# - External API calls from clusters
#
# ğŸ”§ Solutions:
# 1. Set up internal package mirrors (PyPI, Maven, Docker registry)
# 2. Pre-install libraries via init scripts from internal storage
# 3. Use workspace libraries (uploaded to workspace)
# 4. Configure custom repositories in cluster configuration
#
# ğŸ“š References:
# - Private Link setup: https://learn.microsoft.com/en-us/azure/databricks/security/network/private-link
# - Air-gapped deployments: https://learn.microsoft.com/en-us/azure/databricks/security/network/classic/customer-managed-vpc
# - Library management: https://learn.microsoft.com/en-us/azure/databricks/libraries/
#
# ==============================================
