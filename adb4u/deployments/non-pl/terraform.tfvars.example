# ==============================================
# Non-PL Deployment Configuration Example
# ==============================================
# This file provides example values for deploying an Azure Databricks workspace
# using the Non-PL pattern (Control Plane + NPIP Data Plane).
#
# Copy this file to terraform.tfvars and customize for your environment.
# ==============================================

# ==============================================
# Authentication Configuration
# ==============================================
# Terraform providers support multiple authentication methods.
# Choose the method that best fits your environment.

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ OPTION 1: Environment Variables (RECOMMENDED)                            │
# │ Best for: Development, CI/CD pipelines, credential security              │
# └──────────────────────────────────────────────────────────────────────────┘
#
# Add to ~/.zshrc or ~/.bashrc:
#
# # Azure Authentication
# export ARM_CLIENT_ID="<service-principal-app-id>"
# export ARM_CLIENT_SECRET="<service-principal-secret>"
# export ARM_TENANT_ID="<azure-ad-tenant-id>"
# export ARM_SUBSCRIPTION_ID="<azure-subscription-id>"
#
# # Databricks Account Authentication
# export TF_VAR_databricks_account_id="<databricks-account-id>"
#
# Then run: source ~/.zshrc

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ OPTION 2: Azure CLI (DEVELOPMENT ONLY)                                   │
# │ Best for: Local development, testing                                     │
# └──────────────────────────────────────────────────────────────────────────┘
#
# Prerequisites:
# 1. Install Azure CLI: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli
# 2. Login: az login
# 3. Set subscription: az account set --subscription <subscription-id>
# 4. Verify: az account show
#
# Terraform will automatically use your az cli credentials.
# You only need to set databricks_account_id below.

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ OPTION 3: Service Principal (PRODUCTION)                                 │
# │ Best for: Production, CI/CD, automated deployments                       │
# └──────────────────────────────────────────────────────────────────────────┘
#
# Create Service Principal:
# az ad sp create-for-rbac \
#   --name "terraform-databricks-sp" \
#   --role "Contributor" \
#   --scopes /subscriptions/<subscription-id>
#
# Add User Access Administrator role (needed for RBAC):
# az role assignment create \
#   --assignee "<app-id>" \
#   --role "User Access Administrator" \
#   --scope /subscriptions/<subscription-id>
#
# Then set environment variables (see OPTION 1)

# ==============================================
# Core Configuration (REQUIRED)
# ==============================================

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ DEPLOYMENT MODE (Master Control) - NEW!                                  │
# └──────────────────────────────────────────────────────────────────────────┘

use_byor_infrastructure = false  # Set to true if using deployments/byor

# When TRUE:  Uses infrastructure from deployments/byor
#             - Network: VNet, subnets, NSG (from BYOR)
#             - NAT Gateway: Already created (automatic)
#             - CMK: Key Vault from BYOR (if CMK enabled)
#             - Copy-paste BYOR output: terraform output copy_paste_config
#
# When FALSE: Creates all resources from scratch (default)
#             - Must configure: vnet_address_space, subnet_address_prefix
#             - Must set: enable_nat_gateway = true
#             - If CMK: Workspace creates new Key Vault

workspace_prefix = "proddb"  # Lowercase alphanumeric, max 12 chars
location         = "eastus2"
resource_group_name = "rg-databricks-prod-eastus2"

# Databricks Account ID (REQUIRED for Unity Catalog)
# Get from: https://accounts.azuredatabricks.net (top-right corner)
# Format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
databricks_account_id = ""

# ==============================================
# Network Configuration
# ==============================================

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ SCENARIO 1: Create New Network (use_byor_infrastructure = false)         │
# └──────────────────────────────────────────────────────────────────────────┘

vnet_address_space           = ["10.100.0.0/16"]
public_subnet_address_prefix = ["10.100.1.0/26"]
private_subnet_address_prefix = ["10.100.2.0/26"]
enable_nat_gateway = true  # Required for downloading packages (PyPI, Maven, etc.)

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ SCENARIO 2: Use BYOR Infrastructure (use_byor_infrastructure = true)     │
# └──────────────────────────────────────────────────────────────────────────┘

# Run this from BYOR deployment to get copy-paste config:
#   cd deployments/byor
#   terraform output copy_paste_config
#
# Then paste the output here. Example:
#
# existing_vnet_name            = "proddb-vnet-9a8b"
# existing_resource_group_name  = "rg-databricks-byor-eastus2"
# existing_public_subnet_name   = "proddb-public-subnet-9a8b"
# existing_private_subnet_name  = "proddb-private-subnet-9a8b"
# existing_nsg_name             = "proddb-nsg-9a8b"
#
# Note: NAT Gateway flag is automatically managed when using BYOR

# ==============================================
# Service Endpoint Policy (Security)
# ==============================================

enable_service_endpoint_policy = true  # Restricts storage access to approved accounts (Recommended)

# ==============================================
# Customer-Managed Keys (Optional)
# ==============================================

enable_cmk_managed_services = false  # Control plane data (notebooks, secrets, queries)
enable_cmk_managed_disks    = false  # Cluster VM managed disks
enable_cmk_dbfs_root        = false  # Workspace DBFS root storage

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ SCENARIO 1: Create New Key Vault (use_byor_infrastructure = false)       │
# └──────────────────────────────────────────────────────────────────────────┘

# Workspace will automatically create Key Vault (no additional config needed)
# Note: Set enable_cmk_* flags to true above

# ┌──────────────────────────────────────────────────────────────────────────┐
# │ SCENARIO 2: Use BYOR Key Vault (use_byor_infrastructure = true)          │
# └──────────────────────────────────────────────────────────────────────────┘

# Copy-paste from BYOR output (if BYOR created Key Vault):
#   terraform output copy_paste_config
#
# Example:
# existing_key_vault_id = "/subscriptions/.../vaults/proddb-kv-9a8b"
# existing_key_id       = "https://proddb-kv-9a8b.vault.azure.net/keys/databricks-cmk/..."
#
# Note: Key Vault creation is automatically managed when using BYOR

# Requirements:
# - Key Vault must be in same region as workspace
# - Key Vault must have purge protection enabled (auto-configured if creating new)
# - Soft delete retention: 90 days (auto-configured if creating new)
# - Key auto-rotation: 90 days (auto-configured if creating new)

# ==============================================
# IP Access Lists (Optional)
# ==============================================

enable_ip_access_lists = false
# allowed_ip_ranges = [
#   "203.0.113.0/24",    # Corporate network
#   "198.51.100.0/24"    # VPN range
# ]

# ==============================================
# Service Endpoint Policy (Optional)
# ==============================================

# Enable SEP to restrict VNet egress to only allow-listed storage accounts
# Applies to: Classic compute only (not serverless)
# Security: Prevents data exfiltration via unauthorized storage access
enable_service_endpoint_policy = false

# IMPORTANT: Workspace Requirement for SEP
# -----------------------------------------
# Workspaces created on or after July 14, 2025 support SEP by default.
# 
# For workspaces created BEFORE July 14, 2025:
#   1. Contact your Databricks account team to enable SEP support
#   2. Once enabled, set enable_service_endpoint_policy = true
#   3. Run terraform apply
#
# If SEP is not enabled and you set this to true, you'll see this error:
#   "ServiceResourceNameMustHaveEvenElements: invalid resource name /services/Azure/Databricks"
#
# Reference: https://learn.microsoft.com/en-us/azure/databricks/security/network/classic/service-endpoints

# Optional: Additional customer storage accounts to allow
# Example: Existing data lakes, backup storage, shared storage accounts
# additional_allowed_storage_ids = [
#   "/subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/data-rg/providers/Microsoft.Storage/storageAccounts/mydatalake",
#   "/subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/backup-rg/providers/Microsoft.Storage/storageAccounts/mybackups"
# ]

# ==============================================
# Unity Catalog Configuration
# ==============================================

# First-time deployment: Create new metastore
create_metastore = true
metastore_name   = "prod-eastus2-metastore"

# Subsequent workspaces in same region: Reuse existing metastore
# create_metastore      = false
# existing_metastore_id = "abc123-def456-ghi789"

# Access Connector (Managed Identity)
# Option 1: Create per-workspace (Default - Recommended)
create_access_connector = true

# Option 2: Share existing Access Connector across workspaces
# create_access_connector               = false
# existing_access_connector_id          = "/subscriptions/.../resourceGroups/.../providers/Microsoft.Databricks/accessConnectors/shared-connector"
# existing_access_connector_principal_id = "abcd1234-ef56-7890-ghij-klmnopqrstuv"

# ==============================================
# Tags
# ==============================================

tag_owner     = "your.email@company.com"
tag_keepuntil = "12/31/2025"

tags = {
  Environment = "Production"
  ManagedBy   = "Terraform"
  Pattern     = "Non-PL"
  CostCenter  = "Engineering"
  Owner       = "data-platform-team@company.com"
}
