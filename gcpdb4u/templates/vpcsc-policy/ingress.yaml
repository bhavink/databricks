################################################################################
# DATABRICKS GCP VPC SERVICE CONTROLS - OPERATIONAL INGRESS POLICIES
################################################################################
#
# PURPOSE:
# These policies define ingress rules for Databricks workspaces AFTER creation.
# They allow Databricks Control Plane and services to manage and operate your
# workspaces, clusters, and data storage.
#
# USAGE STAGE: Phase 2 - Post-Workspace Creation
# - Apply this policy AFTER workspace is successfully created
# - Use together with egress.yaml for complete operational security
# - Replaces create-ws-ingress.yaml
#
# IMPORTANT NOTES:
# - Update all [WORKSPACEID], [GEO], [REGION] placeholders with actual values
# - Update all project numbers to match your deployment
# - Add Unity Catalog service accounts if using Unity Catalog
# - Add BigQuery rules if using Lakehouse Federation
# - Add CMEK rules if using Customer-Managed Encryption Keys
#
# DOCUMENTATION REFERENCES:
# - VPC Service Controls: https://docs.gcp.databricks.com/en/security/network/vpc-sc.html
# - Regional Resources: https://docs.databricks.com/gcp/en/resources/ip-domain-region
# - Unity Catalog: https://docs.databricks.com/gcp/en/sql/language-manual/sql-ref-syntax-aux-describe-credential
#
################################################################################

################################################################################
# INGRESS RULE 1: Workspace Operations
#
# PURPOSE: Allows Databricks Control Plane to manage workspace compute resources
#
# TRAFFIC FLOW: Databricks Regional Control Plane → Customer Project
#
# WHAT THIS ALLOWS:
# - Launch and manage GCE cluster instances
# - Manage compute disks and operations
# - Read network and firewall configurations
# - Attach service accounts to clusters (for workload identity)
# - Enable required Google APIs
#
# IDENTITIES:
# - Consumer SA: db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com
#   This is the workspace-specific service account created during workspace creation
# - Optional: Add user or group identities for direct access
#
# SOURCE PROJECTS:
# - Databricks Regional Control Plane project (specific to your region)
#
# REQUIRED UPDATES:
# - Replace db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com
#   with your actual Consumer SA (find it in IAM after workspace creation)
# - Replace projects/[CONTROL-PLANE-PROJECT-NUMBER] with your region's control plane
# - Replace projects/546577047680 with your workspace service project number
# - Replace projects/610371902002 with your VPC/host project number
################################################################################
- ingressFrom:
    identities:
    # Workspace-specific Consumer Service Account
    # Created during workspace creation - format: db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com
    # Example: db-311716749948597@prod-gcp-us-east1.iam.gserviceaccount.com
      - serviceAccount:db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com

    # Optional: Additional identities for direct access
    # Uncomment if you need workspace admins to manage resources directly:
     #- group:databricks-admins@corp.com     # Google Group
     #- user:admin@corp.com                  # Individual user

    sources:
    # Databricks Regional Control Plane Project
    # Find your region's project number here:
    # https://docs.databricks.com/gcp/en/resources/ip-domain-region#private-service-connect-psc-attachment-uris-and-project-numbers
    #
    # Example project numbers by region:
    # - us-east1: 51066298900
    # - us-east4: 121886670913
    # - us-central1: 121886670913
    # - europe-west2: 216164888453
      - resource: projects/[CONTROL-PLANE-PROJECT-NUMBER]  # UPDATE: Your region's control plane

  ingressTo:
    operations:
    ############################################################################
    # Compute Engine API Operations
    # Required for: Cluster lifecycle management (create, delete, monitor)
    ############################################################################
    - methodSelectors:
      # Network operations - Read VPC network configuration
      - method: NetworksService.Get

      # Project operations - Get project metadata
      - method: ProjectsService.Get

      # Firewall operations - Read firewall rules
      - method: FirewallsService.Get

      # Subnet operations - Read and manage subnet IAM policies
      - method: SubnetworksService.Get
      - method: SubnetworksService.SetPolicy    # Set IAM policy on subnet
      - method: SubnetworksService.GetPolicy    # Read IAM policy from subnet

      # Instance operations - Full cluster instance lifecycle management
      - method: InstancesService.List           # List all compute instances
      - method: InstancesService.Get            # Get specific instance details
      - method: InstancesService.BulkInsert     # Create multiple instances (cluster nodes)
      - method: InstancesService.Delete         # Terminate cluster instances
      - method: InstancesService.SetLabels      # Add labels to instances for tracking

      # Disk operations - Manage persistent disks attached to cluster nodes
      - method: DisksService.List               # List disks
      - method: DisksService.Get                # Get disk details
      - method: DisksService.SetLabels          # Add labels to disks

      # Zone/Region operations - Query available zones and regions
      - method: ZonesService.List               # List available zones
      - method: RegionsService.List             # List available regions
      - method: RegionsService.Get              # Get region details

      # Operation tracking - Monitor async operation progress
      - method: GlobalOperationsService.Get     # Track global operations
      - method: RegionOperationsService.List    # List regional operations
      - method: ZoneOperationsService.List      # List zonal operations
      - method: ZoneOperationsService.Get       # Get operation status
      serviceName: compute.googleapis.com

    ############################################################################
    # Service Usage API Operations
    # Required for: Verifying enabled Google APIs
    ############################################################################
    - methodSelectors:
      # Allow all methods - VPC-SC doesn't support method filtering for this API
      # Primary call: google.api.serviceusage.v1.ServiceUsage.ListServices
      # Verifies required APIs (compute, storage, iam) are enabled
      - method: '*'
      serviceName: serviceusage.googleapis.com

    ############################################################################
    # Cloud Resource Manager API Operations
    # Required for: Attaching service accounts to clusters
    ############################################################################
    - methodSelectors:
      # Set IAM policy - Required for:
      # 1. Attaching databricks-compute GSA to classic compute clusters
      # 2. Workload Identity feature (custom SA attachment to clusters)
      # Reference: https://docs.databricks.com/gcp/en/compute/configure#google-service-account
      - method: Projects.SetIamPolicy
      serviceName: cloudresourcemanager.googleapis.com

    ############################################################################
    # IAM API Operations
    # Required for: Reading service account and IAM policy information
    ############################################################################
    - methodSelectors:
      # Read IAM policies - Get current IAM policies on resources
      - method: IAM.GetIamPolicy

      # Service account operations - Read service account details
      # Used when attaching service accounts to clusters
      - method: IAM.GetServiceAccount
      serviceName: iam.googleapis.com

    resources:
    # Target projects where these operations are allowed
    - projects/546577047680  # UPDATE: Your workspace service project number
    - projects/610371902002  # UPDATE: Your VPC/host project number

################################################################################
# INGRESS RULE 2: Storage Operations (DBFS and Unity Catalog)
#
# PURPOSE: Allows Databricks services to access Cloud Storage for DBFS and Unity Catalog
#
# TRAFFIC FLOW: Databricks Services → Customer Cloud Storage
#
# WHAT THIS ALLOWS:
# - Read/write/delete objects in DBFS buckets
# - Unity Catalog metadata storage access
# - Serverless compute storage access
#
# IDENTITIES:
# - Consumer SA: db-[WORKSPACEID]@... (workspace-specific)
# - Unity Catalog SA: db-uc-credential-*@uc-[REGION].iam.gserviceaccount.com
#   (Find UC credentials: DESCRIBE CREDENTIAL in Databricks SQL)
#
# SOURCE PROJECTS:
# - Databricks Regional Control Plane project (classic compute)
# - Unity Catalog regional project (Unity Catalog storage)
# - Serverless Compute regional project (serverless SQL/DLT)
#
# STORAGE BUCKETS ACCESSED:
# - databricks-[WORKSPACEID] (main DBFS bucket)
# - databricks-[WORKSPACEID]-system (system bucket)
# - Unity Catalog managed storage buckets
# - User-created external storage buckets
#
# REQUIRED UPDATES:
# - Replace db-[WORKSPACEID]@... with your actual Consumer SA
# - Replace db-uc-credential-*@... with your Unity Catalog credential SA(s)
# - Update project numbers for control plane, Unity Catalog, and serverless
# - Update target project number (where storage buckets reside)
################################################################################
- ingressFrom:
    identities:
    # Workspace Consumer Service Account
    # Manages DBFS and workspace-level storage access
    - serviceAccount:db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com

    # Unity Catalog Credential Service Accounts
    # One or more may exist depending on your Unity Catalog configuration
    # Find your UC credentials with: DESCRIBE CREDENTIAL <credential_name>
    # Format: db-uc-credential-<ID>-<SUFFIX>@uc-<region>.iam.gserviceaccount.com
    - serviceAccount:db-uc-credential-06akrlmcak-xn@uc-useast1.iam.gserviceaccount.com
    # Add additional UC credentials as needed:
    # - serviceAccount:db-uc-credential-<ID2>-<SUFFIX2>@uc-<region>.iam.gserviceaccount.com

    sources:
    # Databricks project sources where storage calls originate
    # Reference: https://docs.databricks.com/gcp/en/resources/ip-domain-region

    # Databricks Regional Control Plane - Classic compute storage access
    # Example: us-east1 = 121886670913
    - resource: projects/121886670913  # UPDATE: Regional control plane project

    # Unity Catalog Regional Project - Unity Catalog storage operations
    # Example: us-east1 UC = 522339604799
    - resource: projects/522339604799  # UPDATE: Unity Catalog regional project

    # Serverless Compute Regional Project - Serverless SQL/DLT storage
    # Example: us-east1 serverless = 748623325325
    - resource: projects/748623325325  # UPDATE: Serverless compute project

  ingressTo:
    operations:
    ############################################################################
    # Cloud Storage API Operations
    # Required for: DBFS access, Unity Catalog storage, user bucket access
    ############################################################################
    - methodSelectors:
      # Object operations - Read, write, delete files in storage buckets
      - method: google.storage.objects.get      # Read objects (files)
      - method: google.storage.objects.list     # List objects in bucket
      - method: google.storage.objects.create   # Write new objects
      - method: google.storage.objects.delete   # Delete objects

      # Bucket operations - Read bucket metadata and check permissions
      - method: google.storage.buckets.get              # Get bucket metadata
      - method: google.storage.buckets.testIamPermissions  # Check bucket access
      serviceName: storage.googleapis.com

    resources:
    # Target project where storage buckets are located
    # Typically the workspace service project or VPC project
    - projects/610371902002  # UPDATE: Project number where buckets reside

################################################################################
# INGRESS RULE 3: Lakehouse Federation to BigQuery (Optional)
#
# PURPOSE: Allows Databricks Serverless Compute to access BigQuery for Lakehouse Federation
#
# TRAFFIC FLOW: Databricks Serverless Compute → Customer BigQuery
#
# WHAT THIS ALLOWS:
# - Query BigQuery tables from Databricks notebooks/SQL
# - Read BigQuery datasets and table metadata
# - Create, update, delete BigQuery tables from Databricks
# - Execute BigQuery jobs from Databricks
#
# WHEN TO USE:
# - If you use Lakehouse Federation to query BigQuery data sources
# - If you run federated queries across Databricks and BigQuery
# - Skip this rule if you don't use BigQuery integration
#
# IDENTITIES:
# - Custom Google Service Account with BigQuery permissions
#   (You must create this SA and grant it BigQuery access)
#
# SOURCE PROJECTS:
# - Serverless Compute regional project
#
# REQUIRED UPDATES:
# - Replace [YOUR-GSA-HAVING-BQ-ACCESS]@[PROJECT].iam.gserviceaccount.com
#   with your actual service account that has BigQuery permissions
# - Update serverless compute project number
# - Update target project number (where BigQuery datasets reside)
# - Adjust method selectors based on your required BigQuery operations
################################################################################
- ingressFrom:
    identities:
    # Custom Service Account with BigQuery Access
    # You must create this SA and grant appropriate BigQuery IAM roles:
    # - roles/bigquery.dataViewer (read access)
    # - roles/bigquery.jobUser (execute queries)
    # - roles/bigquery.dataEditor (write access, if needed)
    - serviceAccount:[YOUR-GSA-HAVING-BQ-ACCESS]@[PROJECT].iam.gserviceaccount.com

    sources:
    # Serverless Compute Regional Project
    # BigQuery queries from Databricks run through serverless compute
    # Example: us-east1 serverless = 748623325325
    - resource: projects/748623325325  # UPDATE: Serverless compute project

  ingressTo:
    operations:
    ############################################################################
    # BigQuery API Operations
    # Required for: Lakehouse Federation queries to BigQuery
    ############################################################################
    - methodSelectors:
    # IMPORTANT: Adjust these methods based on your use case
    # Below is a sample rule - add/remove methods as needed

      # BigQuery Storage Read API - High-performance table reads
      - method: 'BigQueryRead.CreateReadSession'  # Create read session for efficient reads

      # Table operations - Read and manage BigQuery tables
      - method: 'bigquery.tables.get'             # Get table metadata
      - method: 'bigquery.tables.getData'         # Read table data
      - method: 'bigquery.tables.list'            # List tables in dataset
      - method: 'bigquery.tables.create'          # Create new tables
      - method: 'bigquery.tables.update'          # Update table schema
      - method: 'bigquery.tables.delete'          # Delete tables
      - method: 'bigquery.tables.updateData'      # Insert/update table data

      # Dataset operations - Manage BigQuery datasets
      - method: 'bigquery.datasets.get'           # Get dataset metadata
      - method: 'bigquery.datasets.create'        # Create datasets

      # Job operations - Execute BigQuery queries
      - method: 'bigquery.jobs.create'            # Run queries and jobs
      serviceName: bigquery.googleapis.com

    resources:
    # Target project where BigQuery datasets reside
    - projects/610371902002  # UPDATE: Project with BigQuery datasets

################################################################################
# INGRESS RULE 4: Audit Log Delivery (Optional)
#
# PURPOSE: Allows Databricks to deliver audit logs to your Cloud Storage bucket
#
# TRAFFIC FLOW: Databricks Audit Log Service → Customer Storage Bucket
#
# WHAT THIS ALLOWS:
# - Write audit logs to your designated storage bucket
# - Read and delete existing log files (for log rotation/management)
# - Check bucket permissions before writing
#
# WHEN TO USE:
# - If you configured audit log delivery to a customer-managed GCS bucket
# - Reference: https://docs.gcp.databricks.com/en/admin/account-settings/audit-log-delivery.html
# - Skip this rule if you don't use audit log delivery
#
# IDENTITIES:
# - log-delivery@databricks-prod-master.iam.gserviceaccount.com
#   (Databricks-owned service account for log delivery)
#
# SOURCE PROJECTS:
# - Databricks audit log delivery projects (2 global projects)
#
# STORAGE BUCKET:
# - Your designated audit log storage bucket (configured in Databricks account settings)
#
# REQUIRED UPDATES:
# - Update target project number (where audit log bucket resides)
################################################################################
- ingressFrom:
    identities:
    # Databricks Log Delivery Service Account
    # This is a Databricks-owned SA - DO NOT CHANGE
    - serviceAccount:log-delivery@databricks-prod-master.iam.gserviceaccount.com

    sources:
    # Databricks Audit Log Delivery Projects
    # These are global Databricks projects - DO NOT CHANGE THESE NUMBERS
    - resource: projects/68422481410   # Databricks audit log delivery project 1
    - resource: projects/85638097580   # Databricks audit log delivery project 2

  ingressTo:
    operations:
    ############################################################################
    # Cloud Storage API Operations
    # Required for: Writing audit logs to customer bucket
    ############################################################################
    - methodSelectors:
      # Object operations - Write, read, delete audit log files
      - method: google.storage.objects.create       # Write new audit log files
      - method: google.storage.objects.delete       # Delete old log files (rotation)
      - method: google.storage.objects.get          # Read existing log files
      - method: google.storage.objects.list         # List log files in bucket

      # Bucket operations - Check permissions before writing
      - method: google.storage.buckets.testIamPermissions  # Verify write access
      serviceName: storage.googleapis.com

    resources:
    # Target project where audit log storage bucket resides
    # This is typically your workspace service project
    - projects/546577047680  # UPDATE: Project with audit log bucket

################################################################################
# INGRESS RULE 5: Customer-Managed Encryption Keys (CMEK) (Optional)
#
# PURPOSE: Allows Databricks to use customer-managed encryption keys for DBFS
#
# TRAFFIC FLOW: Databricks Control Plane → Customer Cloud KMS
#
# WHAT THIS ALLOWS:
# - Encrypt/decrypt data using your Cloud KMS keys
# - Access key rings and crypto keys for DBFS encryption
#
# WHEN TO USE:
# - If you configured Customer-Managed Encryption Keys (CMEK) for DBFS
# - Reference: https://docs.databricks.com/gcp/en/security/keys/customer-managed-keys
# - Skip this rule if you use Google-managed encryption (default)
#
# IDENTITIES:
# - Consumer SA: db-[WORKSPACEID]@...
#
# SOURCE PROJECTS:
# - Databricks Regional Control Plane project
#
# KMS OPERATIONS:
# - All methods allowed because VPC-SC doesn't support granular KMS method filtering
# - Actual operations: encrypt, decrypt, getKeyRing, getCryptoKey, getPublicKey
#
# REQUIRED UPDATES:
# - Replace db-[WORKSPACEID]@... with your Consumer SA
# - Update control plane project number
# - Update target project number (where KMS keys reside)
################################################################################
- ingressFrom:
    identities:
    # Workspace Consumer Service Account
    # This SA needs cloudkms.cryptoKeyEncrypterDecrypter role on your KMS keys
    - serviceAccount:db-[WORKSPACEID]@prod-gcp-[GEO]-[REGION].iam.gserviceaccount.com

    sources:
    # Databricks Regional Control Plane
    # CMEK operations originate from control plane
    - resource: projects/121886670913  # UPDATE: Regional control plane project

  ingressTo:
    operations:
    ############################################################################
    # Cloud KMS API Operations
    # Required for: Encrypting/decrypting DBFS data with customer keys
    ############################################################################
    - methodSelectors:
      # Allow all methods - VPC-SC doesn't support method selectors for KMS
      # Required operations include:
      # - cloudkms.cryptoKeys.encrypt: Encrypt data before storage
      # - cloudkms.cryptoKeys.decrypt: Decrypt data when accessed
      # - cloudkms.keyRings.get: Access key ring metadata
      # - cloudkms.cryptoKeys.get: Access crypto key metadata
      # - cloudkms.cryptoKeys.getPublicKey: For asymmetric keys
      - method: '*'
      serviceName: cloudkms.googleapis.com

    resources:
    # Target project where Cloud KMS keys reside
    # Typically your workspace service project or separate KMS project
    - projects/610371902002  # UPDATE: Project with KMS keys

################################################################################
# CONFIGURATION SUMMARY:
#
# This ingress policy file includes 5 rule groups:
# 1. ✅ Workspace Operations (REQUIRED) - Cluster management and compute operations
# 2. ✅ Storage Operations (REQUIRED) - DBFS and Unity Catalog storage access
# 3. ⚠️  BigQuery Federation (OPTIONAL) - Only if using Lakehouse Federation
# 4. ⚠️  Audit Log Delivery (OPTIONAL) - Only if configured in account settings
# 5. ⚠️  CMEK Operations (OPTIONAL) - Only if using customer-managed encryption
#
# NEXT STEPS:
#
# 1. Update all placeholders:
#    - [WORKSPACEID], [GEO], [REGION] - From your workspace details
#    - Project numbers - From Databricks regional documentation
#    - Unity Catalog SAs - From DESCRIBE CREDENTIAL command
#
# 2. Remove optional rules if not needed:
#    - Remove BigQuery rule if not using Lakehouse Federation
#    - Remove Audit Log rule if not configured
#    - Remove CMEK rule if using default encryption
#
# 3. Apply together with egress.yaml:
#    - Test in dry-run mode first
#    - Monitor VPC-SC logs for violations
#    - Enforce after successful testing
#
# 4. Test workspace functionality:
#    - Launch clusters and verify they start successfully
#    - Access DBFS: dbutils.fs.ls("dbfs:/")
#    - Test Unity Catalog access (if configured)
#    - Verify audit logs appear (if configured)
#
# TROUBLESHOOTING:
# - Check VPC-SC logs: gcloud logging read "protoPayload.metadata.@type=type.googleapis.com/google.cloud.audit.VpcServiceControlAuditMetadata"
# - Common issues:
#   • Wrong project numbers → Verify from Databricks regional documentation
#   • Missing Unity Catalog SAs → Run DESCRIBE CREDENTIAL to get correct SA
#   • Storage access denied → Verify Consumer SA has Storage Object Admin role
################################################################################
